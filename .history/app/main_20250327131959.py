from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_extract, col, avg, count

# Initialisation de Spark
spark = SparkSession.builder.appName("WebLogAnalysis").getOrCreate()

# D√©finition du sch√©ma des logs
log_pattern = r'(\S+) - - \[(.*?)\] "(\S+) (\S+) HTTP/1.1" (\d+) (\d+)'

# Chargement des logs
import os

data_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../data/access.log"))

if not os.path.exists(data_path):
    raise FileNotFoundError(f"Le fichier {data_path} est introuvable. V√©rifie son emplacement.")

df = spark.read.text(data_path)

# Extraction des donn√©es importantes
logs_df = df.select(
    regexp_extract("value", log_pattern, 1).alias("ip"),
    regexp_extract("value", log_pattern, 2).alias("timestamp"),
    regexp_extract("value", log_pattern, 3).alias("method"),
    regexp_extract("value", log_pattern, 4).alias("url"),
    regexp_extract("value", log_pattern, 5).cast("integer").alias("status"),
    regexp_extract("value", log_pattern, 6).cast("integer").alias("size")
)

# Calcul des m√©triques
print("üöÄ Top 5 pages les plus visit√©es")
logs_df.groupBy("url").count().orderBy(col("count").desc()).show(5)

print("üìä Temps de r√©ponse moyen")
logs_df.select(avg("size").alias("Taille moyenne des r√©ponses")).show()

print("‚ùå Nombre d'erreurs 500")
logs_df.filter(col("status") == 500).agg(count("*").alias("Total Erreurs 500")).show()

# Arr√™ter Spark
spark.stop()
